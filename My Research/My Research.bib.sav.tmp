@Article{Jordan2022,
  author     = {Jordan, Jeremy},
  journal    = {arXiv preprint arXiv:2201.03898},
  title      = {Introduction to autoencoders},
  year       = {2022},
  abstract   = {In this article, we will look at autoencoders. This article covers the mathematics and the fundamental concepts of autoencoders. We will discuss what they are, what the limitations are, the typical use cases, and we will look at some examples. We will start with a general introduction to autoencoders, and we will discuss the role of the activation function in the output layer and the loss function. We will then discuss what the reconstruction error is. Finally, we will look at typical applications as dimensionality reduction, classification, denoising, and anomaly detection. This paper contains the notes of a PhD-level lecture on autoencoders given in 2021.},
  eprint     = {2201.03898},
  file       = {:Jordan-2022-Introduction to autoencoders.pdf:PDF},
  groups     = {Autoencoder, Machine Learning, Model},
  priority   = {prio3},
  ranking    = {rank2},
  readstatus = {read},
  refid      = {35},
  shorttitle = {Introduction to autoencoders},
}

﻿
@InProceedings{Andermatt2018,
  code       = {https://github.com/zubata88/mdgru},
  author     = {Andermatt, Simon and Pezold, Simon and Cattin, Philippe C.},
  booktitle  = {Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries},
  title      = {Automated Segmentation of Multiple Sclerosis Lesions Using Multi-dimensional Gated Recurrent Units},
  year       = {2018},
  address    = {Cham},
  editor     = {Crimi, Alessandro and Bakas, Spyridon and Kuijf, Hugo and Menze, Bjoern and Reyes, Mauricio},
  pages      = {31-42},
  publisher  = {Springer International Publishing},
  abstract   = {We analyze the performance of multi-dimensional gated recurrent units on automated lesion segmentation in multiple sclerosis. The segmentation of these pathologic structures is not trivial, since location, shape and size can be arbitrary. Furthermore, the inherent class imbalance of about 1 lesion voxel to 10 000 healthy voxels further exacerbates the correct segmentation. We introduce a new MD-GRU setup, using established techniques from the deep learning community as well as our own adaptations. We evaluate these modifications by comparing them to a standard MD-GRU network. We demonstrate that using data augmentation, selective sampling, residual learning and/or DropConnect on the RNN state can produce better segmentation results. Reaching rank #1 in the ISBI 2015 longitudinal multiple sclerosis lesion segmentation challenge, we show that a setup which combines these techniques can outperform the state of the art in automated lesion segmentation.},
  doi        = {https://doi.org/10.1007/978-3-319-75238-9_3},
  file       = {:Andermatt-2018-Automated Segmentation of Multi.pdf:PDF},
  groups     = {Gated Recurrent Unit, Model, Classification},
  isbn       = {978-3-319-75238-9},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {read},
  type       = {Conference Proceedings},
}

@Article{Andermatt2017,
  author     = {Andermatt, Simon and Pezold, Simon and Amann, Michael and Cattin, Philippe C.},
  journal    = {arXiv preprint arXiv:1708.02766},
  title      = {Multi-dimensional Gated Recurrent Units for Automated Anatomical Landmark Localization},
  year       = {2017},
  code       = {https://github.com/zubata88/mdgru},
  abstract   = {We analyze the performance of multi-dimensional gated recurrent units on automated lesion segmentation in multiple sclerosis. The segmentation of these pathologic structures is not trivial, since location, shape and size can be arbitrary. Furthermore, the inherent class imbalance of about 1 lesion voxel to 10 000 healthy voxels further exacerbates the correct segmentation. We introduce a new MD-GRU setup, using established techniques from the deep learning community as well as our own adaptations. We evaluate these modifications by comparing them to a standard MD-GRU network. We demonstrate that using data augmentation, selective sampling, residual learning and/or DropConnect on the RNN state can produce better segmentation results. Reaching rank #1 in the ISBI 2015 longitudinal multiple sclerosis lesion segmentation challenge, we show that a setup which combines these techniques can outperform the state of the art in automated lesion segmentation.},
  eprint     = {1708.02766},
  file       = {:Andermatt-2017-Multi-dimensional gated recurre.pdf:PDF},
  groups     = {Gated Recurrent Unit, Model, Classification},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {read},
  type       = {Journal Article},
}

﻿
@Article{Zhao2020,
  author   = {Zhao, Tong and Liu, Yozen and Neves, Leonardo and Woodford, Oliver and Jiang, Meng and Shah, Neil},
  journal  = {arXiv preprint arXiv:2006.06830},
  title    = {Data augmentation for graph neural networks},
  year     = {2020},
  code     = {https://github.com/zhao-tong/GAug},
  abstract = {Data augmentation has been widely used to improve generalizability of machine learning models. However, comparatively little work studies data augmentation for graphs. This is largely due to the complex, non-Euclidean structure of graphs, which limits possible manipulation operations. Augmentation operations commonly used in vision and language have no analogs for graphs. Our work studies graph data augmentation for graph neural networks (GNNs) in the context of improving semi-supervised node-classification. We discuss practical and theoretical motivations, considerations and strategies for graph data augmentation. Our work shows that neural edge predictors can effectively encode class-homophilic structure to promote intra-class edges and demote inter-class edges in given graph structure, and our main contribution introduces the GAug graph data augmentation framework, which leverages these insights to improve performance in GNN-based node classification via edge prediction. Extensive experiments on multiple benchmarks show that augmentation via GAug improves performance across GNN architectures and datasets.},
  eprint   = {2006.06830},
  file     = {:Zhao-2020-Data augmentation for graph neural n.pdf:PDF},
  groups   = {Graph Neural Network},
  priority = {prio3},
  ranking  = {rank2},
  type     = {Journal Article},
}

﻿
@Article{Yang2022,
  author   = {Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal  = {arXiv preprint arXiv:2203.03466},
  title    = {Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
  year     = {2022},
  code     = {https://github.com/microsoft/mup},
  abstract = {Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify muTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7% of total pretraining cost.},
  eprint   = {2203.03466},
  file     = {:Yang-2022-Tensor Programs V_ Tuning Large Neur.pdf:PDF},
  groups   = {Natural Language Processing},
  priority = {prio3},
  ranking  = {rank1},
  type     = {Journal Article},
}

﻿
@Article{BinKamilin2021,
  author     = {Bin Kamilin, Mohd Hafizuddin and Bin Ahmadon, Mohd Anuaruddin and Yamaguchi, Shingo},
  journal    = {Information},
  title      = {Multi-Task Learning-Based Task Scheduling Switcher for a Resource-Constrained IoT System},
  year       = {2021},
  code       = {https://github.com/hafiz-kamilin/research_taskSchedulingSwitcher},
  number     = {4},
  pages      = {150},
  volume     = {12},
  abstract   = {In this journal, we proposed a novel method of using multi-task learning to switch the scheduling algorithm. With multi-task learning to change the scheduling algorithm inside the scheduling framework, the scheduling framework can create a scheduler with the best task execution optimization under the computation deadline. With the changing number of tasks, the number of types of resources taken, and computation deadline, it is hard for a single scheduling algorithm to achieve the best scheduler optimization while avoiding the worst-case time complexity in a resource-constrained Internet of Things (IoT) system due to the trade-off in computation time and optimization in each scheduling algorithm. Furthermore, different hardware specifications affect the scheduler computation time differently, making it hard to rely on Big-O complexity as a reference. With multi-task learning to profile the scheduling algorithm behavior on the hardware used to compute the scheduler, we can identify the best scheduling algorithm. Our benchmark result shows that it can achieve an average of 93.68% of accuracy in meeting the computation deadline, along with 23.41% of average optimization. Based on the results, our method can improve the scheduling of the resource-constrained IoT system.},
  doi        = {10.3390/info12040150},
  file       = {:Bin Kamilin-2021-Multi-Task Learning-Based Tas.pdf:PDF},
  groups     = {Multi-Task Learning, Scheduling, Internet of Things},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {read},
  type       = {Journal Article},
}

@Article{Ruder2017,
  author     = {Ruder, Sebastian},
  journal    = {arXiv preprint arXiv:1706.05098},
  title      = {An overview of multi-task learning in deep neural networks},
  year       = {2017},
  abstract   = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
  eprint     = {1706.05098},
  file       = {:Ruder-2017-An overview of multi-task learning.pdf:PDF},
  groups     = {Multi-Task Learning},
  priority   = {prio2},
  ranking    = {rank4},
  readstatus = {read},
  type       = {Journal Article},
}

@Article{Li2022,
  author   = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julia and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agusti and Hubert, Thomas and Choy, Peter and d'Autume, Cyprien de Masson and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Robson, Esme Sutherland and Kohli, Pushmeet and Freitas, Nando de and Kavukcuoglu, Koray and Vinyals, Oriol},
  journal  = {DeepMind 2022},
  title    = {Competition-Level Code Generation with AlphaCode},
  year     = {2022},
  code     = {https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode},
  abstract = {Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.},
  file     = {:Yujia Li-2022-Competition-Level Code Generatio.pdf:PDF},
  groups   = {Natural Language Processing, Translation},
  priority = {prio1},
  ranking  = {rank4},
  type     = {Journal Article},
  url      = {https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode},
}

﻿
@Article{Biessmann2019,
  author   = {Biessmann, Felix and Rukat, Tammo and Schmidt, Philipp and Naidu, Prathik and Schelter, Sebastian and Taptunov, Andrey and Lange, Dustin and Salinas, David},
  journal  = {J. Mach. Learn. Res.},
  title    = {DataWig: Missing Value Imputation for Tables},
  year     = {2019},
  code     = {https://github.com/awslabs/datawig},
  number   = {175},
  pages    = {1-6},
  volume   = {20},
  abstract = {With the growing importance of machine learning (ML) algorithms for practical applications, reducing data quality problems in ML pipelines has become a major focus of research. In many cases missing values can break data pipelines which makes completeness one of the most impactful data quality challenges. Current missing value imputation methods are focusing on numerical or categorical data and can be difficult to scale to datasets with millions of rows. We release DataWig, a robust and scalable approach for missing value imputation that can be applied to tables with heterogeneous data types, including unstructured text. DataWig combines deep learning feature extractors with automatic hyperparameter tuning. This enables users without a machine learning background, such as data engineers, to impute missing values with minimal effort in tables with more heterogeneous data types than supported in existing libraries, while requiring less glue code for feature engineering and offering more flexible modelling options. We demonstrate that DataWig compares favourably to existing imputation packages. Source code, documentation, and unit tests for this package are available at: https://github.com/awslabs/datawig},
  file     = {:Biessmann-2019-DataWig_ Missing Value Imputati.pdf:PDF},
  groups   = {Missing Data},
  priority = {prio2},
  ranking  = {rank3},
  type     = {Journal Article},
}

﻿
@Article{Whittington2021,
  author   = {Whittington, James CR and Warren, Joseph and Behrens, Timothy EJ},
  journal  = {arXiv preprint arXiv:2112.04035},
  title    = {Relating transformers to models and neural representations of the hippocampal formation},
  year     = {2021},
  abstract = {Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain. One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind. In this work, we show that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place 