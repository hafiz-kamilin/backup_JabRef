@Article{Jordan2022_IntroductiontoAutoencoders,
  author     = {Jordan, Jeremy},
  journal    = {arXiv preprint arXiv:2201.03898},
  title      = {Introduction to autoencoders},
  year       = {2022},
  abstract   = {In this article, we will look at autoencoders. This article covers the mathematics and the fundamental concepts of autoencoders. We will discuss what they are, what the limitations are, the typical use cases, and we will look at some examples. We will start with a general introduction to autoencoders, and we will discuss the role of the activation function in the output layer and the loss function. We will then discuss what the reconstruction error is. Finally, we will look at typical applications as dimensionality reduction, classification, denoising, and anomaly detection. This paper contains the notes of a PhD-level lecture on autoencoders given in 2021.},
  eprint     = {2201.03898},
  file       = {:Jordan-2022-Introduction to autoencoders.pdf:PDF},
  groups     = {Autoencoder, Machine Learning},
  priority   = {prio3},
  ranking    = {rank2},
  readstatus = {read},
  refid      = {35},
  shorttitle = {Introduction to autoencoders},
}

﻿
@InProceedings{Andermatt2018_AutomatedSegmentationofMultipleSclerosisLesionsUsingMultiDimensionalGatedRecurrentUnits,
  author     = {Andermatt, Simon and Pezold, Simon and Cattin, Philippe C.},
  booktitle  = {Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries},
  title      = {Automated Segmentation of Multiple Sclerosis Lesions Using Multi-dimensional Gated Recurrent Units},
  year       = {2018},
  code       = {https://github.com/zubata88/mdgru},
  address    = {Cham},
  editor     = {Crimi, Alessandro and Bakas, Spyridon and Kuijf, Hugo and Menze, Bjoern and Reyes, Mauricio},
  pages      = {31-42},
  publisher  = {Springer International Publishing},
  abstract   = {We analyze the performance of multi-dimensional gated recurrent units on automated lesion segmentation in multiple sclerosis. The segmentation of these pathologic structures is not trivial, since location, shape and size can be arbitrary. Furthermore, the inherent class imbalance of about 1 lesion voxel to 10 000 healthy voxels further exacerbates the correct segmentation. We introduce a new MD-GRU setup, using established techniques from the deep learning community as well as our own adaptations. We evaluate these modifications by comparing them to a standard MD-GRU network. We demonstrate that using data augmentation, selective sampling, residual learning and/or DropConnect on the RNN state can produce better segmentation results. Reaching rank #1 in the ISBI 2015 longitudinal multiple sclerosis lesion segmentation challenge, we show that a setup which combines these techniques can outperform the state of the art in automated lesion segmentation.},
  doi        = {https://doi.org/10.1007/978-3-319-75238-9_3},
  file       = {:Andermatt-2018-Automated Segmentation of Multi.pdf:PDF},
  groups     = {Gated Recurrent Unit, Classification},
  isbn       = {978-3-319-75238-9},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {read},
  type       = {Conference Proceedings},
}

@Article{Andermatt2017_MultiDimensionalGatedRecurrentUnitsforAutomatedAnatomicalLandmarkLocalization,
  author     = {Andermatt, Simon and Pezold, Simon and Amann, Michael and Cattin, Philippe C.},
  journal    = {arXiv preprint arXiv:1708.02766},
  title      = {Multi-dimensional Gated Recurrent Units for Automated Anatomical Landmark Localization},
  year       = {2017},
  code       = {https://github.com/zubata88/mdgru},
  abstract   = {We analyze the performance of multi-dimensional gated recurrent units on automated lesion segmentation in multiple sclerosis. The segmentation of these pathologic structures is not trivial, since location, shape and size can be arbitrary. Furthermore, the inherent class imbalance of about 1 lesion voxel to 10 000 healthy voxels further exacerbates the correct segmentation. We introduce a new MD-GRU setup, using established techniques from the deep learning community as well as our own adaptations. We evaluate these modifications by comparing them to a standard MD-GRU network. We demonstrate that using data augmentation, selective sampling, residual learning and/or DropConnect on the RNN state can produce better segmentation results. Reaching rank #1 in the ISBI 2015 longitudinal multiple sclerosis lesion segmentation challenge, we show that a setup which combines these techniques can outperform the state of the art in automated lesion segmentation.},
  eprint     = {1708.02766},
  file       = {:Andermatt-2017-Multi-dimensional gated recurre.pdf:PDF},
  groups     = {Gated Recurrent Unit, Classification},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {read},
  type       = {Journal Article},
}

﻿
@Article{Zhao2020_DataAugmentationforGraphNeuralNetworks,
  author   = {Zhao, Tong and Liu, Yozen and Neves, Leonardo and Woodford, Oliver and Jiang, Meng and Shah, Neil},
  journal  = {arXiv preprint arXiv:2006.06830},
  title    = {Data augmentation for graph neural networks},
  year     = {2020},
  code     = {https://github.com/zhao-tong/GAug},
  abstract = {Data augmentation has been widely used to improve generalizability of machine learning models. However, comparatively little work studies data augmentation for graphs. This is largely due to the complex, non-Euclidean structure of graphs, which limits possible manipulation operations. Augmentation operations commonly used in vision and language have no analogs for graphs. Our work studies graph data augmentation for graph neural networks (GNNs) in the context of improving semi-supervised node-classification. We discuss practical and theoretical motivations, considerations and strategies for graph data augmentation. Our work shows that neural edge predictors can effectively encode class-homophilic structure to promote intra-class edges and demote inter-class edges in given graph structure, and our main contribution introduces the GAug graph data augmentation framework, which leverages these insights to improve performance in GNN-based node classification via edge prediction. Extensive experiments on multiple benchmarks show that augmentation via GAug improves performance across GNN architectures and datasets.},
  eprint   = {2006.06830},
  file     = {:Zhao-2020-Data augmentation for graph neural n.pdf:PDF},
  groups   = {Graph Neural Network},
  priority = {prio3},
  ranking  = {rank2},
  type     = {Journal Article},
}

﻿
@Article{Yang2022_TensorProgramsVTuningLargeNeuralNetworksViaZeroShotHyperparameterTransfer,
  author   = {Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal  = {arXiv preprint arXiv:2203.03466},
  title    = {Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
  year     = {2022},
  code     = {https://github.com/microsoft/mup},
  abstract = {Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify muTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7% of total pretraining cost.},
  eprint   = {2203.03466},
  file     = {:Yang-2022-Tensor Programs V_ Tuning Large Neur.pdf:PDF},
  groups   = {Natural Language Processing, Tuning/Training/Optimizing},
  priority = {prio3},
  ranking  = {rank1},
  type     = {Journal Article},
}

﻿
@Article{BinKamilin2021_MultiTaskLearningBasedTaskSchedulingSwitcherforaResourceConstrainedIoTSystem,
  author     = {Bin Kamilin, Mohd Hafizuddin and Bin Ahmadon, Mohd Anuaruddin and Yamaguchi, Shingo},
  journal    = {Information},
  title      = {Multi-Task Learning-Based Task Scheduling Switcher for a Resource-Constrained IoT System},
  year       = {2021},
  code       = {https://github.com/hafiz-kamilin/research_taskSchedulingSwitcher},
  number     = {4},
  pages      = {150},
  volume     = {12},
  abstract   = {In this journal, we proposed a novel method of using multi-task learning to switch the scheduling algorithm. With multi-task learning to change the scheduling algorithm inside the scheduling framework, the scheduling framework can create a scheduler with the best task execution optimization under the computation deadline. With the changing number of tasks, the number of types of resources taken, and computation deadline, it is hard for a single scheduling algorithm to achieve the best scheduler optimization while avoiding the worst-case time complexity in a resource-constrained Internet of Things (IoT) system due to the trade-off in computation time and optimization in each scheduling algorithm. Furthermore, different hardware specifications affect the scheduler computation time differently, making it hard to rely on Big-O complexity as a reference. With multi-task learning to profile the scheduling algorithm behavior on the hardware used to compute the scheduler, we can identify the best scheduling algorithm. Our benchmark result shows that it can achieve an average of 93.68% of accuracy in meeting the computation deadline, along with 23.41% of average optimization. Based on the results, our method can improve the scheduling of the resource-constrained IoT system.},
  doi        = {10.3390/info12040150},
  file       = {:Bin Kamilin-2021-Multi-Task Learning-Based Tas.pdf:PDF},
  groups     = {Multi-Task Learning, Internet of Things, Scheduler},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {read},
  type       = {Journal Article},
}

@Article{Ruder2017_AnOverviewofMultiTaskLearninginDeepNeuralNetworks,
  author     = {Ruder, Sebastian},
  journal    = {arXiv preprint arXiv:1706.05098},
  title      = {An overview of multi-task learning in deep neural networks},
  year       = {2017},
  abstract   = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
  eprint     = {1706.05098},
  file       = {:Ruder-2017-An overview of multi-task learning.pdf:PDF},
  groups     = {Multi-Task Learning, Classification},
  priority   = {prio2},
  ranking    = {rank4},
  readstatus = {read},
  type       = {Journal Article},
}

@Article{Li2022_CompetitionLevelCodeGenerationwithAlphaCode,
  author   = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julia and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agusti and Hubert, Thomas and Choy, Peter and d'Autume, Cyprien de Masson and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Robson, Esme Sutherland and Kohli, Pushmeet and Freitas, Nando de and Kavukcuoglu, Koray and Vinyals, Oriol},
  journal  = {DeepMind 2022},
  title    = {Competition-Level Code Generation with AlphaCode},
  year     = {2022},
  code     = {https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode},
  abstract = {Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.},
  file     = {:Yujia Li-2022-Competition-Level Code Generatio.pdf:PDF},
  groups   = {Natural Language Processing, Language},
  priority = {prio1},
  ranking  = {rank4},
  type     = {Journal Article},
  url      = {https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode},
}

﻿
@Article{Biessmann2019_DataWigMissingValueImputationforTables,
  author   = {Biessmann, Felix and Rukat, Tammo and Schmidt, Philipp and Naidu, Prathik and Schelter, Sebastian and Taptunov, Andrey and Lange, Dustin and Salinas, David},
  journal  = {J. Mach. Learn. Res.},
  title    = {DataWig: Missing Value Imputation for Tables},
  year     = {2019},
  code     = {https://github.com/awslabs/datawig},
  number   = {175},
  pages    = {1-6},
  volume   = {20},
  abstract = {With the growing importance of machine learning (ML) algorithms for practical applications, reducing data quality problems in ML pipelines has become a major focus of research. In many cases missing values can break data pipelines which makes completeness one of the most impactful data quality challenges. Current missing value imputation methods are focusing on numerical or categorical data and can be difficult to scale to datasets with millions of rows. We release DataWig, a robust and scalable approach for missing value imputation that can be applied to tables with heterogeneous data types, including unstructured text. DataWig combines deep learning feature extractors with automatic hyperparameter tuning. This enables users without a machine learning background, such as data engineers, to impute missing values with minimal effort in tables with more heterogeneous data types than supported in existing libraries, while requiring less glue code for feature engineering and offering more flexible modelling options. We demonstrate that DataWig compares favourably to existing imputation packages. Source code, documentation, and unit tests for this package are available at: https://github.com/awslabs/datawig},
  file     = {:Biessmann-2019-DataWig_ Missing Value Imputati.pdf:PDF},
  groups   = {Missing Data},
  priority = {prio2},
  ranking  = {rank3},
  type     = {Journal Article},
  url      = {https://jmlr.org/papers/v20/18-753.html},
}

﻿
@Article{Whittington2021_RelatingTransformerstoModelsandNeuralRepresentationsoftheHippocampalFormation,
  author   = {Whittington, James CR and Warren, Joseph and Behrens, Timothy EJ},
  journal  = {arXiv preprint arXiv:2112.04035},
  title    = {Relating transformers to models and neural representations of the hippocampal formation},
  year     = {2021},
  abstract = {Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain. One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind. In this work, we show that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place and grid cells. Furthermore, we show that this result is no surprise since it is closely related to current hippocampal models from neuroscience. We additionally show the transformer version offers dramatic performance gains over the neuroscience version. This work continues to bind computations of artificial and brain networks, offers a novel understanding of the hippocampal-cortical interaction, and suggests how wider cortical areas may perform complex tasks beyond current neuroscience models such as language comprehension.},
  eprint   = {2112.04035},
  file     = {:Whittington-2021-Relating transformers to mode.pdf:PDF},
  groups   = {Miscellaneous},
  priority = {prio3},
  ranking  = {rank1},
  type     = {Journal Article},
}

﻿
@InProceedings{Wang2021_RealEsrganTrainingRealWorldBlindSuperResolutionwithPureSyntheticData,
  author    = {Wang, Xintao and Xie, Liangbin and Dong, Chao and Shan, Ying},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  title     = {Real-esrgan: Training real-world blind super-resolution with pure synthetic data},
  year      = {2021},
  code      = {https://github.com/xinntao/Real-ESRGAN},
  pages     = {1905-1914},
  abstract  = {Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly.},
  doi       = {10.1109/iccvw54120.2021.00217},
  file      = {:Wang-2021-Real-esrgan_ Training real-world bli.pdf:PDF},
  groups    = {Generative adversarial networks, Upscaler, Upscaling Image/Video, Tuning/Training/Optimizing},
  priority  = {prio2},
  ranking   = {rank3},
  type      = {Conference Proceedings},
}

﻿
@Article{Anderson2022_FindingtheOptimalHumanStrategyforWordleUsingMaximumCorrectLetterProbabilitiesandReinforcementLearning,
  author   = {Anderson, Benton J and Meyer, Jesse G},
  journal  = {arXiv preprint arXiv:2202.00557},
  title    = {Finding the optimal human strategy for Wordle using maximum correct letter probabilities and reinforcement learning},
  year     = {2022},
  abstract = {Wordle is an online word puzzle game that gained viral popularity in January 2022. The goal is to guess a hidden five letter word. After each guess, the player gains information about whether the letters they guessed are present in the word, and whether they are in the correct position. Numerous blogs have suggested guessing strategies and starting word lists that improve the chance of winning. Optimized algorithms can win 100% of games within five of the six allowed trials. However, it is infeasible for human players to use these algorithms due to an inability to perfectly recall all known 5-letter words and perform complex calculations that optimize information gain. Here, we present two different methods for choosing starting words along with a framework for discovering the optimal human strategy based on reinforcement learning. Human Wordle players can use the rules we discover to optimize their chance of winning.},
  eprint   = {2202.00557},
  file     = {:Anderson-2022-Finding the optimal human strate.pdf:PDF},
  groups   = {Reinforcement Learning, Prediction},
  priority = {prio3},
  ranking  = {rank2},
  type     = {Journal Article},
}

﻿
@Article{Pan2022_MachineLearningBasedWhiteHatWormLauncherinBotnetDefenseSystem,
  author   = {Pan, Xiangnan and Yamaguchi, Shingo and Kageyama, Taku and Kamilin, Mohd Hafizuddin Bin},
  journal  = {International Journal of Software Science and Computational Intelligence},
  title    = {Machine-Learning-Based White-Hat Worm Launcher in Botnet Defense System},
  year     = {2022},
  issn     = {1942-9045
1942-9037},
  number   = {1},
  pages    = {1-14},
  volume   = {14},
  abstract = {This article proposes a white-hat worm launcher based on machine learning (ML) adaptable to large-scale IoT network for Botnet Defense System (BDS). BDS is a cyber-security system that uses white-hat worms to exterminate malicious botnets. White-hat worms defend an IoT system against malicious bots, the BDS decides the number of white-hat worms, but there is no discussion on the white-hat worms' deployment in IoT network. Therefore, the authors propose a machine-learning-based launcher to launch the white-hat worms effectively along with a divide and conquer algorithm to deploy the launcher to large-scale IoT networks. Then the authors modeled BDS and the launcher with agent-oriented Petri net and confirmed the effect through the simulation of the PN2 model. The result showed that the proposed launcher can reduce the number of infected devices by about 30-40%.},
  doi      = {10.4018/ijssci.291713},
  file     = {:Pan-2022-Machine-Learning-Based White-Hat Worm.pdf:PDF},
  groups   = {Multi-Task Learning, Internet Security, Deep Neural Network},
  priority = {prio1},
  ranking  = {rank5},
  type     = {Journal Article},
  url      = {https://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/IJSSCI.291713},
}

@Article{LeCun2022_APathTowardsAutonomousMachineIntelligenceVersion0.9.220220627,
  author   = {LeCun, Yann},
  title    = {A Path Towards Autonomous Machine Intelligence Version 0.9. 2, 2022-06-27},
  year     = {2022},
  abstract = {How could machines learn as efficiently as humans and animals? How could machines learn to reason and plan? How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict,
and plan at multiple time horizons? This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as configurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.},
  file     = {:LeCun-2022-a_path_towards_autonomous_mach.pdf:PDF},
  groups   = {Artificial General Intelligence},
  priority = {prio2},
  ranking  = {rank4},
  url      = {https://openreview.net/forum?id=BZ5a1r-kVsf},
}

﻿
@Sourcecode{Peng2019_Anime4KaHighQualityRealTimeUpscalerforAnimeVideo,
  author         = {Peng, B.},
  title          = {Anime4K: A High-Quality Real Time Upscaler for Anime Video},
  year           = {2019},
  code           = {https://github.com/bloc97/Anime4K},
  lastupdatedate = {26 November 2021},
  abstract       = {Anime4K is a set of open-source, high-quality real-time anime upscaling/denoising algorithms that can be implemented in any programming language.

The simplicity and speed of Anime4K allows the user to watch upscaled anime in real time, as we believe in preserving original content and promoting freedom of choice for all anime fans. Re-encoding anime into 4K should be avoided as it is non-reversible, potentially damages original content by introducing artifacts, takes up to O(n^2) more disk space and more importantly, does so without any meaningful decrease in entropy (lost information is lost).},
  file           = {:Peng-2019-Anime4K_ A High-Quality Real Time Up.pdf:PDF},
  groups         = {Upscaler, Upscaling Image/Video},
  month          = {11/26/2021},
  priority       = {prio3},
  publisher      = {Github},
  ranking        = {rank2},
  readstatus     = {read},
  type           = {Sourcecode},
  url            = {https://github.com/bloc97/Anime4K},
}

﻿
@InProceedings{Dong_AcceleratingtheSuperResolutionConvolutionalNeuralNetwork,
  author    = {Dong, Chao and Loy, Chen Change and Tang, Xiaoou},
  booktitle = {European conference on computer vision},
  title     = {Accelerating the super-resolution convolutional neural network},
  year      = {2016},
  code      = {https://github.com/yjn870/FSRCNN-pytorch, https://github.com/Saafke/FSRCNN_Tensorflow},
  pages     = {391-407},
  publisher = {Springer},
  abstract  = {As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt smaller filter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with even superior restoration quality. Further, we present the parameter settings that can achieve real-time performance on a generic CPU while still maintaining good performance. A corresponding transfer strategy is also proposed for fast training and testing across different upscaling factors.},
  doi       = {10.1007/978-3-319-46475-6_25},
  file      = {:Dong-2016-Accelerating the super-resolution co.pdf:PDF},
  groups    = {Upscaler, Upscaling Image/Video},
  priority  = {prio2},
  ranking   = {rank3},
  type      = {Conference Proceedings},
}

﻿
@Article{Pan2021_IncoherentReconstructionFreeObjectRecognitionwithMaskBasedLenslessOpticsandtheTransformer,
  author   = {Pan, Xiuxi and Chen, Xiao and Nakamura, Tomoya and Yamaguchi, Masahiro},
  journal  = {Optics Express},
  title    = {Incoherent reconstruction-free object recognition with mask-based lensless optics and the Transformer},
  year     = {2021},
  number   = {23},
  pages    = {37962-37978},
  volume   = {29},
  abstract = {A mask-based lensless camera adopts a thin mask to optically encode the scene and records the encoded pattern on an image sensor. The lensless camera can be thinner, lighter and cheaper than the lensed camera. But additional computation is required to reconstruct an image from the encoded pattern. Considering that the significant application of the lensless camera could be inference, we propose to perform object recognition directly on the encoded pattern. Avoiding image reconstruction not only saves computational resources but also averts errors and artifacts in reconstruction. We theoretically analyze multiplexing property in mask-based lensless optics which maps local information in the scene to overlapping global information in the encoded pattern. To better extract global features, we propose a simplified Transformer-based architecture. This is the first time to study Transformer-based architecture for encoded pattern recognition in mask-based lensless optics. In the optical experiment, the proposed system achieves 91.47&#x0025; accuracy on the Fashion MNIST and 96.64&#x0025; ROC AUC on the cats-vs-dogs dataset. The feasibility of physical object recognition is also evaluated.},
  doi      = {10.1364/OE.443181},
  file     = {:Pan-2022-Image reconstruction with transformer.pdf:PDF},
  groups   = {Reconstruction},
  keywords = {Feature extraction Image reconstruction Image sensors Optical sensing Pattern recognition Stochastic gradient descent},
  priority = {prio2},
  ranking  = {rank3},
  type     = {Journal Article},
  url      = {http://opg.optica.org/oe/abstract.cfm?URI=oe-29-23-37962},
}

@Article{Pan2022_ImageReconstructionwithTransformerforMaskBasedLenslessImaging,
  author   = {Pan, Xiuxi and Chen, Xiao and Takeyama, Saori and Yamaguchi, Masahiro},
  journal  = {Optics Letters},
  title    = {Image reconstruction with transformer for mask-based lensless imaging},
  year     = {2022},
  number   = {7},
  pages    = {1843-1846},
  volume   = {47},
  abstract = {A mask-based lensless camera optically encodes the scene with a thin mask and reconstructs the image afterward. The improvement of image reconstruction is one of the most important subjects in lensless imaging. Conventional model-based reconstruction approaches, which leverage knowledge of the physical system, are susceptible to imperfect system modeling. Reconstruction with a pure data-driven deep neural network (DNN) avoids this limitation, thereby having potential to provide a better reconstruction quality. However, existing pure DNN reconstruction approaches for lensless imaging do not provide a better result than model-based approaches. We reveal that the multiplexing property in lensless optics makes global features essential in understanding the optically encoded pattern. Additionally, all existing DNN reconstruction approaches apply fully convolutional networks (FCNs) which are not efficient in global feature reasoning. With this analysis, for the first time to the best of our knowledge, a fully connected neural network with a transformer for image reconstruction is proposed. The proposed architecture is better in global feature reasoning, and hence enhances the reconstruction. The superiority of the proposed architecture is verified by comparing with the model-based and FCN-based approaches in an optical experiment.},
  doi      = {10.1364/OL.455378},
  file     = {:Pan-2021-Incoherent reconstruction-free object.pdf:PDF},
  groups   = {Reconstruction},
  keywords = {Fresnel zones Image quality Image reconstruction Image sensors Neural networks Reconstruction algorithms},
  priority = {prio2},
  ranking  = {rank3},
  type     = {Journal Article},
  url      = {http://opg.optica.org/ol/abstract.cfm?URI=ol-47-7-1843},
}

﻿
@Article{Makridakis2020_TheM4Competition100000TimeSeriesand61ForecastingMethods,
  author   = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  journal  = {International Journal of Forecasting},
  title    = {The M4 Competition: 100,000 time series and 61 forecasting methods},
  year     = {2020},
  issn     = {0169-2070},
  number   = {1},
  pages    = {54-74},
  volume   = {36},
  abstract = {The M4 Competition follows on from the three previous M competitions, the purpose of which was to learn from empirical evidence both how to improve the forecasting accuracy and how such learning could be used to advance the theory and practice of forecasting. The aim of M4 was to replicate and extend the three previous competitions by: (a) significantly increasing the number of series, (b) expanding the number of forecasting methods, and (c) including prediction intervals in the evaluation process as well as point forecasts. This paper covers all aspects of M4 in detail, including its organization and running, the presentation of its results, the top-performing methods overall and by categories, its major findings and their implications, and the computational requirements of the various methods. Finally, it summarizes its main conclusions and states the expectation that its series will become a testing ground for the evaluation of new methods and the improvement of the practice of forecasting, while also suggesting some ways forward for the field.},
  doi      = {https://doi.org/10.1016/j.ijforecast.2019.04.014},
  file     = {:Makridakis-2020-The M4 Competition_ 100,000 ti.pdf:PDF},
  groups   = {Prediction},
  keywords = {Forecasting competitions M competitions Forecasting accuracy Prediction intervals Time series methods Machine learning methods Benchmarking methods Practice of forecasting},
  priority = {prio1},
  ranking  = {rank4},
  type     = {Journal Article},
  url      = {https://www.sciencedirect.com/science/article/pii/S0169207019301128},
}

@Article{Makridakis2021_TheM5CompetitionBackgroundOrganizationandImplementation,
  author   = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  journal  = {International Journal of Forecasting},
  title    = {The M5 competition: Background, organization, and implementation},
  year     = {2021},
  issn     = {01692070},
  abstract = {The M5 competition follows the previous four M competitions, whose purpose is to learn from empirical evidence how to improve forecasting performance and advance the theory and practice of forecasting. M5 focused on a retail sales forecasting application with the objective to produce the most accurate point forecasts for 42,840 time series that represent the hierarchical unit sales of the largest retail company in the world, Walmart, as well as to provide the most accurate estimates of the uncertainty of these forecasts. Hence, the competition consisted of two parallel challenges, namely the Accuracy and Uncertainty forecasting competitions. M5 extended the results of the previous M competitions by: (a) significantly expanding the number of participating methods, especially those in the category of machine learning; (b) evaluating the performance of the uncertainty distribution along with point forecast accuracy; (c) including exogenous/explanatory variables in addition to the time series data; (d) using grouped, correlated time series; and (e) focusing on series that display intermittency. This paper describes the background, organization, and implementations of the competition, and it presents the data used and their characteristics. Consequently, it serves as introductory material to the results of the two forecasting challenges to facilitate their understanding.},
  doi      = {10.1016/j.ijforecast.2021.07.007},
  file     = {:Makridakis-2021-The M5 competition_ Background.pdf:PDF},
  groups   = {Prediction},
  keywords = {Forecasting competitions M competitions Accuracy Uncertainty Time series Retail sales forecasting},
  priority = {prio1},
  ranking  = {rank4},
  type     = {Journal Article},
  url      = {https://www.sciencedirect.com/science/article/pii/S0169207021001187},
}

﻿
@Article{Wang2022_CLIPGENLanguageFreeTrainingofaTexttoImageGeneratorwithCLIP,
  author   = {Wang, Zihao and Liu, Wei and He, Qian and Wu, Xinglong and Yi, Zili},
  journal  = {arXiv preprint arXiv:2203.00386},
  title    = {CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP},
  year     = {2022},
  abstract = {Training a text-to-image generator in the general domain (e.g., Dall.e, CogView) requires huge amounts of paired text-image data, which is too expensive to collect. In this paper, we propose a self-supervised scheme named as CLIP-GEN for general text-to-image generation with the language-image priors extracted with a pre-trained CLIP model. In our approach, we only require a set of unlabeled images in the general domain to train a text-to-image generator. Specifically, given an image without text labels, we first extract the embedding of the image in the united language-vision embedding space with the image encoder of CLIP. Next, we convert the image into a sequence of discrete tokens in the VQGAN codebook space (the VQGAN model can be trained with the unlabeled image dataset in hand). Finally, we train an autoregressive transformer that maps the image tokens from its unified language-vision representation. Once trained, the transformer can generate coherent image tokens based on the text embedding extracted from the text encoder of CLIP upon an input text. Such a strategy enables us to train a strong and general text-to-image generator with large text-free image dataset such as ImageNet. Qualitative and quantitative evaluations verify that our method significantly outperforms optimization-based text-to-image methods in terms of image quality while not compromising the text-image matching. Our method can even achieve comparable performance as flagship supervised models like CogView.},
  eprint   = {2203.00386},
  file     = {:Wang-2022-CLIP-GEN_ Language-Free Training of.pdf:PDF},
  groups   = {Generative adversarial networks, Generator},
  priority = {prio3},
  ranking  = {rank2},
  type     = {Journal Article},
}

﻿
@Sourcecode{Nixtla2021_NeuralForecast,
  author   = {Nixtla},
  title    = {NeuralForecast},
  year     = {2021},
  abstract = {NeuralForecast is a Python library for time series forecasting with deep learning models. It includes benchmark datasets, data-loading utilities, evaluation functions, statistical tests, univariate model benchmarks and SOTA models implemented in PyTorch and PyTorchLightning.},
  file     = {:GitHub - Nixtla_neuralforecast_ Scalable and.html:URL},
  groups   = {Deep Neural Network},
  month    = {2/28/2022},
  priority = {prio2},
  ranking  = {rank4},
  type     = {Sourcecode},
  url      = {https://nixtla.github.io/neuralforecast/},
}

@Article{Hatamizadeh2022_GlobalContextVisionTransformers,
  author    = {Hatamizadeh, Ali and Yin, Hongxu and Kautz, Jan and Molchanov, Pavlo},
  journal   = {arXiv preprint arXiv:2206.09959},
  title     = {Global Context Vision Transformers},
  year      = {2022},
  code      = {https://github.com/NVlabs/GCViT},
  abstract  = {We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization. Our method leverages global context self-attention modules, joint with local self-attention, to effectively yet efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the issue of lack of the inductive bias in ViTs via proposing to use a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the base, small and tiny variants of GC ViT with 28M, 51M and 90M parameters achieve 83.2%, 83.9% and 84.4% Top-1 accuracy, respectively, surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based Swin Transformer by a large margin. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation using MS COCO and ADE20K datasets outperform prior work consistently, sometimes by large margins.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  doi       = {10.48550/ARXIV.2206.09959},
  file      = {:https___doi.org_10.48550_arxiv.2206.09959 - Global Context Vision Transformers.pdf:PDF},
  groups    = {Tansformer, Classification},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  priority  = {prio2},
  publisher = {arXiv},
  ranking   = {rank3},
  url       = {https://arxiv.org/abs/2206.09959},
}

@Article{Han2022_VisionGNNanImageIsWorthGraphofNodes,
  author   = {Han, Kai and Wang, Yunhe and Guo, Jianyuan and Tang, Yehui and Wu, Enhua},
  journal  = {arXiv preprint arXiv:2206.00272},
  title    = {Vision GNN: An Image is Worth Graph of Nodes},
  year     = {2022},
  code     = {https://github.com/huawei-noah/Efficient-AI-Backbones https://gitee.com/mindspore/models},
  abstract = {Network architecture plays a key role in the deep learning-based computer vision system. The widely-used convolutional neural network and transformer treat the image as a grid or sequence structure, which is not flexible to capture irregular and complex objects. In this paper, we propose to represent the image as a graph structure and introduce a new Vision GNN (ViG) architecture to extract graph-level feature for visual tasks. We first split the image to a number of patches which are viewed as nodes, and construct a graph by connecting the nearest neighbors. Based on the graph representation of images, we build our ViG model to transform and exchange information among all the nodes. ViG consists of two basic modules: Grapher module with graph convolution for aggregating and updating graph information, and FFN module with two linear layers for node feature transformation. Both isotropic and pyramid architectures of ViG are built with different model sizes. Extensive experiments on image recognition and object detection tasks demonstrate the superiority of our ViG architecture. We hope this pioneering study of GNN on general visual tasks will provide useful inspiration and experience for future research.},
  eprint   = {2206.00272},
  file     = {:Han2022_VisionGNNanImageIsWorthGraphofNodes - Vision GNN_ an Image Is Worth Graph of Nodes.pdf:PDF},
  groups   = {Graph Neural Network, Classification},
  priority = {prio2},
  ranking  = {rank3},
}

@Article{Laurencon_TheBigScienceCorpusa1.6TBCompositeMultilingualDataset,
  author   = {Laurençon, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and del Moral, Albert Villanova and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Ponferrada, Eduardo González and Nguyen, Huu},
  journal  = {NeurIPS 2022 Track Datasets and Benchmarks Submission},
  title    = {The BigScience Corpus A 1.6 TB Composite Multilingual Dataset},
  year     = {2022},
  code     = {https://huggingface.co/bigscience-catalogue-lm-data https://github.com/bigscience-workshop/data-preparation},
  abstract = {As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience LargeOpen-science Open-access Multilingual language model (BLOOM). We further release a large initial subset of the corpus and analyses thereof, and hope to empower further large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research into studying this large multilingual corpus.},
  file     = {:the_bigscience_corpus_a_1_6tb_.pdf:PDF},
  groups   = {Natural Language Processing, Language},
  priority = {prio2},
  ranking  = {rank3},
  url      = {https://openreview.net/forum?id=UoEw6KigkUn},
}

@Webpage{Biswal2022_RecurrentNeuralNetworkRNNTutorialTypesandExamples,
  accessdate = {13 July 2022},
  author     = {Biswal, Avijeet},
  publisher  = {Simplilearn},
  title      = {Recurrent Neural Network (RNN) Tutorial: Types and Examples},
  url        = {https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn},
  year       = {2022},
  series     = {Deep Learning Tutorial for Beginners},
  abstract   = {The article explains what is a recurrent neural network, LSTM & types of RNN, why do we need a recurrent neural network, and its applications. Read on for more!},
  file       = {:Recurrent Neural Network (RNN) Tutorial_ Types and Examples.html:URL},
  groups     = {Recurrent Neural Network},
  keywords   = {rnn, recurrent neural network, recurrent neural network tutorial, rnn tutorial, what is rnn, what is recurrent neural network, how does rnn work, lstm, recurrent neural network example},
  priority   = {prio2},
  ranking    = {rank4},
  readstatus = {read},
}

@Webpage{Phi2020_IllustratedGuidetoLSTM’sandGRU’saStepbyStepExplanation,
  accessdate = {14 July 2022},
  author     = {Phi, Michael},
  publisher  = {Toward Data Science},
  title      = {Illustrated Guide to LSTM’s and GRU’s: A step by step explanation},
  url        = {https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21},
  year       = {2020},
  abstract   = {Hi and welcome to an Illustrated Guide to Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU). I’m Michael, and I’m a Machine Learning Engineer in the AI voice assistant space.

In this post, we’ll start with the intuition behind LSTM ’s and GRU’s. Then I’ll explain the internal mechanisms that allow LSTM’s and GRU’s to perform so well. If you want to understand what’s happening under the hood for these two networks, then this post is for you.},
  file       = {:Illustrated Guide to LSTM’s and GRU’s_ A step by step explanation.html:URL},
  groups     = {Long short-term memory},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {skimmed},
  shorttitle = {Illustrated Guide to LSTM’s and GRU’s: A step by step explanation},
}

@Webpage{Olah2015_UnderstandingLSTMNetworks,
  accessdate = {14 July 2022},
  author     = {Olah, Christopher},
  title      = {Understanding LSTM Networks},
  url        = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
  year       = {2015},
  abstract   = {Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.},
  file       = {:Understanding LSTM Networks.html:URL},
  groups     = {Long short-term memory},
  priority   = {prio2},
  ranking    = {rank3},
  shorttitle = {Understanding LSTM Networks},
}

@Webpage{,
  accessdate = {16 July 2022},
  author     = {Haejin Lee, Susan},
  publisher  = {Github},
  title      = {Functional programming is finally going mainstream},
  url        = {https://github.com/readme/featured/functional-programming},
  year       = {2022},
  date       = {12 July 2022},
  abstract   = {Paul Louth had a great development team at Meddbase, the healthcare software company he founded in 2005. But as the company grew, so did their bug count. That’s expected, up to a point. More code and more features mean more defects. But the defect rate was growing faster than Louth expected.

“We were seeing more and more of the same types of bugs,” Louth says. “It was clear that there was an issue, but it wasn’t clear what it was. My instinct told me the problem was complexity.”

Louth and company used C#, which had just happened to add support for Language Integrated Query (LINQ), an alternate language for querying data sources ranging from databases to objects within codebases. LINQ introduced ways to apply the programming paradigm known as “functional programming” to C# code, and learning about how LINQ worked led him to learn more about functional programming in general.},
  file       = {:Functional programming is finally going mainstream.html:URL},
  groups     = {Programming},
  priority   = {prio3},
  ranking    = {rank2},
  readstatus = {read},
}

@Webpage{Bronstein2022_TowardsGeometricDeepLearningIontheShouldersofGiants,
  author     = {Bronstein, Michael},
  publisher  = {Toward Data Science},
  title      = {Towards Geometric Deep Learning I: On the Shoulders of Giants},
  url        = {https://towardsdatascience.com/towards-geometric-deep-learning-i-on-the-shoulders-of-giants-726c205860f5},
  year       = {2022},
  abstract   = {Geometric Deep Learning approaches a broad class of ML problems from the perspectives of symmetry and invariance, providing a common blueprint for neural network architectures as diverse as CNNs, GNNs, and Transformers. In a new series of posts, we study how geometric ideas dating back to ancient Greece have shaped modern deep learning.},
  file       = {:Towards Geometric Deep Learning I_ On the Shoulders of Giants.html:URL},
  groups     = {Miscellaneous},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {read},
  shorttitle = {Towards Geometric Deep Learning II: The Perceptron Affair},
}

@Webpage{Bronstein2022_TowardsGeometricDeepLearningIIIFirstGeometricArchitectures,
  author     = {Bronstein, Michael},
  publisher  = {Toward Data Science},
  title      = {Towards Geometric Deep Learning III: First Geometric Architectures},
  url        = {https://towardsdatascience.com/towards-geometric-deep-learning-iii-first-geometric-architectures-d1578f4ade1f},
  year       = {2022},
  abstract   = {Geometric Deep Learning approaches a broad class of ML problems from the perspectives of symmetry and invariance, providing a common blueprint for neural network architectures as diverse as CNNs, GNNs, and Transformers. In a new series of posts, we study how these ideas have taken us from ancient Greece to convolutional neural networks.},
  file       = {:Towards Geometric Deep Learning III_ First Geometric Architectures.html:URL},
  groups     = {Miscellaneous},
  priority   = {prio2},
  ranking    = {rank3},
  shorttitle = {Towards Geometric Deep Learning II: The Perceptron Affair},
}

@Webpage{Bronstein2022_GeometricFoundationsofDeepLearning,
  author     = {Bronstein, Michael},
  publisher  = {Toward Data Science},
  title      = {Geometric foundations of Deep Learning},
  url        = {https://towardsdatascience.com/geometric-foundations-of-deep-learning-94cdd45b451d},
  year       = {2022},
  abstract   = {Geometric Deep Learning is an attempt for geometric unification of a broad class of ML problems from the perspectives of symmetry and invariance. These principles not only underlie the breakthrough performance of convolutional neural networks and the recent success of graph neural networks but also provide a principled way to construct new types of problem-specific inductive biases.},
  file       = {:Geometric foundations of Deep Learning.html:URL},
  groups     = {Miscellaneous},
  priority   = {prio2},
  ranking    = {rank3},
  shorttitle = {Geometric foundations of Deep Learning},
}

@Book{Atienza2020_AdvancedDeepLearningwithTensorFlow2andKerasApplyDLGANsVAEsDeepRLUnsupervisedLearningObjectDetectionandSegmentationandMore,
  author     = {Atienza, Rowel},
  publisher  = {Packt Publishing Ltd},
  title      = {Advanced Deep Learning with TensorFlow 2 and Keras: Apply DL, GANs, VAEs, deep RL, unsupervised learning, object detection and segmentation, and more},
  year       = {2020},
  chapter    = {Chapter 3: Autoencoders},
  edition    = {2},
  abstract   = {Advanced Deep Learning with TensorFlow 2 and Keras, Second Edition is a completely updated edition of the bestselling guide to the advanced deep learning techniques available today. Revised for TensorFlow 2.x, this edition introduces you to the practical side of deep learning with new chapters on unsupervised learning using mutual information, object detection (SSD), and semantic segmentation (FCN and PSPNet), further allowing you to create your own cutting-edge AI projects.

Using Keras as an open-source deep learning library, the book features hands-on projects that show you how to create more effective AI with the most up-to-date techniques.

Starting with an overview of multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs), the book then introduces more cutting-edge techniques as you explore deep neural network architectures, including ResNet and DenseNet, and how to create autoencoders. You will then learn about GANs, and how they can unlock new levels of AI performance.

Next, you'll discover how a variational autoencoder (VAE) is implemented, and how GANs and VAEs have the generative power to synthesize data that can be extremely convincing to humans. You'll also learn to implement DRL such as Deep Q-Learning and Policy Gradient Methods, which are critical to many modern results in AI.},
  file       = {:Atienza_Advanced Deep Learning with TensorFlow 2 and Keras.pdf:PDF},
  groups     = {Autoencoder},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@Book{Atienza2020_AdvancedDeepLearningwithTensorFlow2andKerasApplyDLGANsVAEsDeepRLUnsupervisedLearningObjectDetectionandSegmentationandMore,
  author     = {Atienza, Rowel},
  publisher  = {Packt Publishing Ltd},
  title      = {Advanced Deep Learning with TensorFlow 2 and Keras: Apply DL, GANs, VAEs, deep RL, unsupervised learning, object detection and segmentation, and more},
  year       = {2020},
  chapter    = {Chapter 1: Introducing Advanced Deep Learning with Keras},
  edition    = {2},
  abstract   = {Advanced Deep Learning with TensorFlow 2 and Keras, Second Edition is a completely updated edition of the bestselling guide to the advanced deep learning techniques available today. Revised for TensorFlow 2.x, this edition introduces you to the practical side of deep learning with new chapters on unsupervised learning using mutual information, object detection (SSD), and semantic segmentation (FCN and PSPNet), further allowing you to create your own cutting-edge AI projects.

Using Keras as an open-source deep learning library, the book features hands-on projects that show you how to create more effective AI with the most up-to-date techniques.

Starting with an overview of multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs), the book then introduces more cutting-edge techniques as you explore deep neural network architectures, including ResNet and DenseNet, and how to create autoencoders. You will then learn about GANs, and how they can unlock new levels of AI performance.

Next, you'll discover how a variational autoencoder (VAE) is implemented, and how GANs and VAEs have the generative power to synthesize data that can be extremely convincing to humans. You'll also learn to implement DRL such as Deep Q-Learning and Policy Gradient Methods, which are critical to many modern results in AI.},
  file       = {:Atienza_Advanced Deep Learning with TensorFlow 2 and Keras.pdf:PDF},
  groups     = {Convolutional Neural Network},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@Book{Atienza2020_AdvancedDeepLearningwithTensorFlow2andKerasApplyDLGANsVAEsDeepRLUnsupervisedLearningObjectDetectionandSegmentationandMorea,
  author     = {Atienza, Rowel},
  publisher  = {Packt Publishing Ltd},
  title      = {Advanced Deep Learning with TensorFlow 2 and Keras: Apply DL, GANs, VAEs, deep RL, unsupervised learning, object detection and segmentation, and more},
  year       = {2020},
  chapter    = {Chapter 2: Deep Neural Networks},
  edition    = {2},
  abstract   = {Advanced Deep Learning with TensorFlow 2 and Keras, Second Edition is a completely updated edition of the bestselling guide to the advanced deep learning techniques available today. Revised for TensorFlow 2.x, this edition introduces you to the practical side of deep learning with new chapters on unsupervised learning using mutual information, object detection (SSD), and semantic segmentation (FCN and PSPNet), further allowing you to create your own cutting-edge AI projects.

Using Keras as an open-source deep learning library, the book features hands-on projects that show you how to create more effective AI with the most up-to-date techniques.

Starting with an overview of multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs), the book then introduces more cutting-edge techniques as you explore deep neural network architectures, including ResNet and DenseNet, and how to create autoencoders. You will then learn about GANs, and how they can unlock new levels of AI performance.

Next, you'll discover how a variational autoencoder (VAE) is implemented, and how GANs and VAEs have the generative power to synthesize data that can be extremely convincing to humans. You'll also learn to implement DRL such as Deep Q-Learning and Policy Gradient Methods, which are critical to many modern results in AI.},
  file       = {:Atienza_Advanced Deep Learning with TensorFlow 2 and Keras.pdf:PDF},
  groups     = {Deep Neural Network},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@Book{Atienza2020_AdvancedDeepLearningwithTensorFlow2andKerasApplyDLGANsVAEsDeepRLUnsupervisedLearningObjectDetectionandSegmentationandMorea,
  author     = {Atienza, Rowel},
  publisher  = {Packt Publishing Ltd},
  title      = {Advanced Deep Learning with TensorFlow 2 and Keras: Apply DL, GANs, VAEs, deep RL, unsupervised learning, object detection and segmentation, and more},
  year       = {2020},
  chapter    = {Chapter 4: Generative Adversarial Networks (GANs)},
  edition    = {2},
  abstract   = {Advanced Deep Learning with TensorFlow 2 and Keras, Second Edition is a completely updated edition of the bestselling guide to the advanced deep learning techniques available today. Revised for TensorFlow 2.x, this edition introduces you to the practical side of deep learning with new chapters on unsupervised learning using mutual information, object detection (SSD), and semantic segmentation (FCN and PSPNet), further allowing you to create your own cutting-edge AI projects.

Using Keras as an open-source deep learning library, the book features hands-on projects that show you how to create more effective AI with the most up-to-date techniques.

Starting with an overview of multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs), the book then introduces more cutting-edge techniques as you explore deep neural network architectures, including ResNet and DenseNet, and how to create autoencoders. You will then learn about GANs, and how they can unlock new levels of AI performance.

Next, you'll discover how a variational autoencoder (VAE) is implemented, and how GANs and VAEs have the generative power to synthesize data that can be extremely convincing to humans. You'll also learn to implement DRL such as Deep Q-Learning and Policy Gradient Methods, which are critical to many modern results in AI.},
  file       = {:Atienza_Advanced Deep Learning with TensorFlow 2 and Keras.pdf:PDF},
  groups     = {Generative adversarial networks},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@Book{Atienza2020_AdvancedDeepLearningwithTensorFlow2andKerasApplyDLGANsVAEsDeepRLUnsupervisedLearningObjectDetectionandSegmentationandMoreb,
  author     = {Atienza, Rowel},
  publisher  = {Packt Publishing Ltd},
  title      = {Advanced Deep Learning with TensorFlow 2 and Keras: Apply DL, GANs, VAEs, deep RL, unsupervised learning, object detection and segmentation, and more},
  year       = {2020},
  chapter    = {Chapter 5: Improved GANs},
  edition    = {2},
  abstract   = {Advanced Deep Learning with TensorFlow 2 and Keras, Second Edition is a completely updated edition of the bestselling guide to the advanced deep learning techniques available today. Revised for TensorFlow 2.x, this edition introduces you to the practical side of deep learning with new chapters on unsupervised learning using mutual information, object detection (SSD), and semantic segmentation (FCN and PSPNet), further allowing you to create your own cutting-edge AI projects.

Using Keras as an open-source deep learning library, the book features hands-on projects that show you how to create more effective AI with the most up-to-date techniques.

Starting with an overview of multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs), the book then introduces more cutting-edge techniques as you explore deep neural network architectures, including ResNet and DenseNet, and how to create autoencoders. You will then learn about GANs, and how they can unlock new levels of AI performance.

Next, you'll discover how a variational autoencoder (VAE) is implemented, and how GANs and VAEs have the generative power to synthesize data that can be extremely convincing to humans. You'll also learn to implement DRL such as Deep Q-Learning and Policy Gradient Methods, which are critical to many modern results in AI.},
  file       = {:Atienza_Advanced Deep Learning with TensorFlow 2 and Keras.pdf:PDF},
  groups     = {Generative adversarial networks},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@Book{Atienza2020_AdvancedDeepLearningwithTensorFlow2andKerasApplyDLGANsVAEsDeepRLUnsupervisedLearningObjectDetectionandSegmentationandMorec,
  author     = {Atienza, Rowel},
  publisher  = {Packt Publishing Ltd},
  title      = {Advanced Deep Learning with TensorFlow 2 and Keras: Apply DL, GANs, VAEs, deep RL, unsupervised learning, object detection and segmentation, and more},
  year       = {2020},
  chapter    = {Chapter 6: Disentangled Representation GANs},
  edition    = {2},
  abstract   = {Advanced Deep Learning with TensorFlow 2 and Keras, Second Edition is a completely updated edition of the bestselling guide to the advanced deep learning techniques available today. Revised for TensorFlow 2.x, this edition introduces you to the practical side of deep learning with new chapters on unsupervised learning using mutual information, object detection (SSD), and semantic segmentation (FCN and PSPNet), further allowing you to create your own cutting-edge AI projects.

Using Keras as an open-source deep learning library, the book features hands-on projects that show you how to create more effective AI with the most up-to-date techniques.

Starting with an overview of multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs), the book then introduces more cutting-edge techniques as you explore deep neural network architectures, including ResNet and DenseNet, and how to create autoencoders. You will then learn about GANs, and how they can unlock new levels of AI performance.

Next, you'll discover how a variational autoencoder (VAE) is implemented, and how GANs and VAEs have the generative power to synthesize data that can be extremely convincing to humans. You'll also learn to implement DRL such as Deep Q-Learning and Policy Gradient Methods, which are critical to many modern results in AI.},
  file       = {:Atienza_Advanced Deep Learning with TensorFlow 2 and Keras.pdf:PDF},
  groups     = {Generative adversarial networks},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@Book{Atienza2020_AdvancedDeepLearningwithTensorFlow2andKerasApplyDLGANsVAEsDeepRLUnsupervisedLearningObjectDetectionandSegmentationandMored,
  author     = {Atienza, Rowel},
  publisher  = {Packt Publishing Ltd},
  title      = {Advanced Deep Learning with TensorFlow 2 and Keras: Apply DL, GANs, VAEs, deep RL, unsupervised learning, object detection and segmentation, and more},
  year       = {2020},
  chapter    = {Chapter 8: Variational Autoencoders (VAEs)},
  edition    = {2},
  abstract   = {Advanced Deep Learning with TensorFlow 2 and Keras, Second Edition is a completely updated edition of the bestselling guide to the advanced deep learning techniques available today. Revised for TensorFlow 2.x, this edition introduces you to the practical side of deep learning with new chapters on unsupervised learning using mutual information, object detection (SSD), and semantic segmentation (FCN and PSPNet), further allowing you to create your own cutting-edge AI projects.

Using Keras as an open-source deep learning library, the book features hands-on projects that show you how to create more effective AI with the most up-to-date techniques.

Starting with an overview of multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs), the book then introduces more cutting-edge techniques as you explore deep neural network architectures, including ResNet and DenseNet, and how to create autoencoders. You will then learn about GANs, and how they can unlock new levels of AI performance.

Next, you'll discover how a variational autoencoder (VAE) is implemented, and how GANs and VAEs have the generative power to synthesize data that can be extremely convincing to humans. You'll also learn to implement DRL such as Deep Q-Learning and Policy Gradient Methods, which are critical to many modern results in AI.},
  file       = {:Atienza_Advanced Deep Learning with TensorFlow 2 and Keras.pdf:PDF},
  groups     = {Autoencoder},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@Book{Atienza2020_AdvancedDeepLearningwithTensorFlow2andKerasApplyDLGANsVAEsDeepRLUnsupervisedLearningObjectDetectionandSegmentationandMored,
  author     = {Atienza, Rowel},
  publisher  = {Packt Publishing Ltd},
  title      = {Advanced Deep Learning with TensorFlow 2 and Keras: Apply DL, GANs, VAEs, deep RL, unsupervised learning, object detection and segmentation, and more},
  year       = {2020},
  chapter    = {Chapter 7: Cross-Domain GANs},
  edition    = {2},
  abstract   = {Advanced Deep Learning with TensorFlow 2 and Keras, Second Edition is a completely updated edition of the bestselling guide to the advanced deep learning techniques available today. Revised for TensorFlow 2.x, this edition introduces you to the practical side of deep learning with new chapters on unsupervised learning using mutual information, object detection (SSD), and semantic segmentation (FCN and PSPNet), further allowing you to create your own cutting-edge AI projects.

Using Keras as an open-source deep learning library, the book features hands-on projects that show you how to create more effective AI with the most up-to-date techniques.

Starting with an overview of multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs), the book then introduces more cutting-edge techniques as you explore deep neural network architectures, including ResNet and DenseNet, and how to create autoencoders. You will then learn about GANs, and how they can unlock new levels of AI performance.

Next, you'll discover how a variational autoencoder (VAE) is implemented, and how GANs and VAEs have the generative power to synthesize data that can be extremely convincing to humans. You'll also learn to implement DRL such as Deep Q-Learning and Policy Gradient Methods, which are critical to many modern results in AI.},
  file       = {:Atienza_Advanced Deep Learning with TensorFlow 2 and Keras.pdf:PDF},
  groups     = {Generative adversarial networks},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@Book{Atienza2020_AdvancedDeepLearningwithTensorFlow2andKerasApplyDLGANsVAEsDeepRLUnsupervisedLearningObjectDetectionandSegmentationandMoree,
  author     = {Atienza, Rowel},
  publisher  = {Packt Publishing Ltd},
  title      = {Advanced Deep Learning with TensorFlow 2 and Keras: Apply DL, GANs, VAEs, deep RL, unsupervised learning, object detection and segmentation, and more},
  year       = {2020},
  chapter    = {Chapter 9: Deep Reinforcement Learning},
  edition    = {2},
  abstract   = {Advanced Deep Learning with TensorFlow 2 and Keras, Second Edition is a completely updated edition of the bestselling guide to the advanced deep learning techniques available today. Revised for TensorFlow 2.x, this edition introduces you to the practical side of deep learning with new chapters on unsupervised learning using mutual information, object detection (SSD), and semantic segmentation (FCN and PSPNet), further allowing you to create your own cutting-edge AI projects.

Using Keras as an open-source deep learning library, the book features hands-on projects that show you how to create more effective AI with the most up-to-date techniques.

Starting with an overview of multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs), the book then introduces more cutting-edge techniques as you explore deep neural network architectures, including ResNet and DenseNet, and how to create autoencoders. You will then learn about GANs, and how they can unlock new levels of AI performance.

Next, you'll discover how a variational autoencoder (VAE) is implemented, and how GANs and VAEs have the generative power to synthesize data that can be extremely convincing to humans. You'll also learn to implement DRL such as Deep Q-Learning and Policy Gradient Methods, which are critical to many modern results in AI.},
  file       = {:Atienza_Advanced Deep Learning with TensorFlow 2 and Keras.pdf:PDF},
  groups     = {Deep Neural Network},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@Webpage{Bronstein2022_TowardsGeometricDeepLearningIIthePerceptronAffair,
  author     = {Bronstein, Michael},
  publisher  = {Toward Data Science},
  title      = {Towards Geometric Deep Learning II: The Perceptron Affair},
  url        = {https://towardsdatascience.com/towards-geometric-deep-learning-ii-the-perceptron-affair-fafa61b5c40a},
  year       = {2022},
  abstract   = {Geometric Deep Learning approaches a broad class of ML problems from the perspectives of symmetry and invariance, providing a common blueprint for neural network architectures as diverse as CNNs, GNNs, and Transformers. In a new series of posts, we study how these ideas have emerged through history from ancient Greek geometry to Graph Neural Networks.},
  file       = {:Towards Geometric Deep Learning II_ The Perceptron Affair.html:URL},
  groups     = {Miscellaneous},
  priority   = {prio2},
  ranking    = {rank3},
  shorttitle = {Towards Geometric Deep Learning II: The Perceptron Affair},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Machine Learning\;2\;1\;0x8a8a8aff\;ROBOT\;Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks. \;;
2 StaticGroup:Application\;2\;0\;0x8a8a8aff\;APPLICATION\;Using machine learning to resolve real world problem.\;;
3 StaticGroup:Classification\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Generator\;0\;1\;0x8a8a8aff\;\;Using machine learning to generate data\;;
3 StaticGroup:Internet of Things\;0\;1\;0x8a8a8aff\;\;Encompass all IoT related problems and the solutions with machine learning.\;;
3 StaticGroup:Internet Security\;0\;1\;0x8a8a8aff\;\;Using machine learning to enhance network security.\;;
3 StaticGroup:Language\;0\;1\;0x8a8a8aff\;\;Using machine learning to translate language.\;;
3 StaticGroup:Missing Data\;0\;1\;0x8a8a8aff\;\;Fill in the missing data in the dataset with machine learning.\;;
3 StaticGroup:Prediction\;0\;1\;0x8a8a8aff\;\;Using machine learning to predict the future.\;;
3 StaticGroup:Reconstruction\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Reconstruction\;0\;0\;0x8a8a8aff\;\;Reconstruct data with artificial intelligence.\;;
3 StaticGroup:Scheduler\;0\;1\;0x8a8a8aff\;\;Using machine learning to solve scheduling problem\;;
3 StaticGroup:Upscaling Image/Video\;0\;1\;0x8a8a8aff\;\;Upscaling image from low resolution to high resolution\;;
2 StaticGroup:Miscellaneous\;0\;1\;0x8a8a8aff\;BOWL_MIX\;\;;
2 StaticGroup:Model\;2\;1\;0x8a8a8aff\;TOY_BRICK\;A machine learning model is a file that has been trained to recognize certain types of patterns.\;;
3 StaticGroup:Artificial General Intelligence\;0\;1\;0x8a8a8aff\;\;Artificial general intelligence (AGI) is the ability of an intelligent agent to understand or learn any intellectual task that a human being can.\;;
3 StaticGroup:Autoencoder\;0\;0\;0x8a8a8aff\;\;An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsu