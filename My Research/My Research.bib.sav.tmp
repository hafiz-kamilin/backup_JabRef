@Article{Jordan2022_IntroductiontoAutoencoders,
  author     = {Jordan, Jeremy},
  journal    = {arXiv preprint arXiv:2201.03898},
  title      = {Introduction to autoencoders},
  year       = {2022},
  abstract   = {In this article, we will look at autoencoders. This article covers the mathematics and the fundamental concepts of autoencoders. We will discuss what they are, what the limitations are, the typical use cases, and we will look at some examples. We will start with a general introduction to autoencoders, and we will discuss the role of the activation function in the output layer and the loss function. We will then discuss what the reconstruction error is. Finally, we will look at typical applications as dimensionality reduction, classification, denoising, and anomaly detection. This paper contains the notes of a PhD-level lecture on autoencoders given in 2021.},
  eprint     = {2201.03898},
  file       = {:Jordan-2022-Introduction to autoencoders.pdf:PDF},
  groups     = {Autoencoder, Machine Learning, Model, Reconstruction},
  priority   = {prio3},
  ranking    = {rank2},
  readstatus = {read},
  refid      = {35},
  shorttitle = {Introduction to autoencoders},
}

﻿
@InProceedings{Andermatt2018_AutomatedSegmentationofMultipleSclerosisLesionsUsingMultiDimensionalGatedRecurrentUnits,
  author     = {Andermatt, Simon and Pezold, Simon and Cattin, Philippe C.},
  booktitle  = {Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries},
  title      = {Automated Segmentation of Multiple Sclerosis Lesions Using Multi-dimensional Gated Recurrent Units},
  year       = {2018},
  code       = {https://github.com/zubata88/mdgru},
  address    = {Cham},
  editor     = {Crimi, Alessandro and Bakas, Spyridon and Kuijf, Hugo and Menze, Bjoern and Reyes, Mauricio},
  pages      = {31-42},
  publisher  = {Springer International Publishing},
  abstract   = {We analyze the performance of multi-dimensional gated recurrent units on automated lesion segmentation in multiple sclerosis. The segmentation of these pathologic structures is not trivial, since location, shape and size can be arbitrary. Furthermore, the inherent class imbalance of about 1 lesion voxel to 10 000 healthy voxels further exacerbates the correct segmentation. We introduce a new MD-GRU setup, using established techniques from the deep learning community as well as our own adaptations. We evaluate these modifications by comparing them to a standard MD-GRU network. We demonstrate that using data augmentation, selective sampling, residual learning and/or DropConnect on the RNN state can produce better segmentation results. Reaching rank #1 in the ISBI 2015 longitudinal multiple sclerosis lesion segmentation challenge, we show that a setup which combines these techniques can outperform the state of the art in automated lesion segmentation.},
  doi        = {https://doi.org/10.1007/978-3-319-75238-9_3},
  file       = {:Andermatt-2018-Automated Segmentation of Multi.pdf:PDF},
  groups     = {Gated Recurrent Unit, Model, Classification},
  isbn       = {978-3-319-75238-9},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {read},
  type       = {Conference Proceedings},
}

@Article{Andermatt2017_MultiDimensionalGatedRecurrentUnitsforAutomatedAnatomicalLandmarkLocalization,
  author     = {Andermatt, Simon and Pezold, Simon and Amann, Michael and Cattin, Philippe C.},
  journal    = {arXiv preprint arXiv:1708.02766},
  title      = {Multi-dimensional Gated Recurrent Units for Automated Anatomical Landmark Localization},
  year       = {2017},
  code       = {https://github.com/zubata88/mdgru},
  abstract   = {We analyze the performance of multi-dimensional gated recurrent units on automated lesion segmentation in multiple sclerosis. The segmentation of these pathologic structures is not trivial, since location, shape and size can be arbitrary. Furthermore, the inherent class imbalance of about 1 lesion voxel to 10 000 healthy voxels further exacerbates the correct segmentation. We introduce a new MD-GRU setup, using established techniques from the deep learning community as well as our own adaptations. We evaluate these modifications by comparing them to a standard MD-GRU network. We demonstrate that using data augmentation, selective sampling, residual learning and/or DropConnect on the RNN state can produce better segmentation results. Reaching rank #1 in the ISBI 2015 longitudinal multiple sclerosis lesion segmentation challenge, we show that a setup which combines these techniques can outperform the state of the art in automated lesion segmentation.},
  eprint     = {1708.02766},
  file       = {:Andermatt-2017-Multi-dimensional gated recurre.pdf:PDF},
  groups     = {Gated Recurrent Unit, Model, Classification},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {read},
  type       = {Journal Article},
}

﻿
@Article{Zhao2020_DataAugmentationforGraphNeuralNetworks,
  author   = {Zhao, Tong and Liu, Yozen and Neves, Leonardo and Woodford, Oliver and Jiang, Meng and Shah, Neil},
  journal  = {arXiv preprint arXiv:2006.06830},
  title    = {Data augmentation for graph neural networks},
  year     = {2020},
  code     = {https://github.com/zhao-tong/GAug},
  abstract = {Data augmentation has been widely used to improve generalizability of machine learning models. However, comparatively little work studies data augmentation for graphs. This is largely due to the complex, non-Euclidean structure of graphs, which limits possible manipulation operations. Augmentation operations commonly used in vision and language have no analogs for graphs. Our work studies graph data augmentation for graph neural networks (GNNs) in the context of improving semi-supervised node-classification. We discuss practical and theoretical motivations, considerations and strategies for graph data augmentation. Our work shows that neural edge predictors can effectively encode class-homophilic structure to promote intra-class edges and demote inter-class edges in given graph structure, and our main contribution introduces the GAug graph data augmentation framework, which leverages these insights to improve performance in GNN-based node classification via edge prediction. Extensive experiments on multiple benchmarks show that augmentation via GAug improves performance across GNN architectures and datasets.},
  eprint   = {2006.06830},
  file     = {:Zhao-2020-Data augmentation for graph neural n.pdf:PDF},
  groups   = {Graph Neural Network},
  priority = {prio3},
  ranking  = {rank2},
  type     = {Journal Article},
}

﻿
@Article{Yang2022_TensorProgramsVTuningLargeNeuralNetworksViaZeroShotHyperparameterTransfer,
  author   = {Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal  = {arXiv preprint arXiv:2203.03466},
  title    = {Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
  year     = {2022},
  code     = {https://github.com/microsoft/mup},
  abstract = {Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify muTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7% of total pretraining cost.},
  eprint   = {2203.03466},
  file     = {:Yang-2022-Tensor Programs V_ Tuning Large Neur.pdf:PDF},
  groups   = {Natural Language Processing, Tuning},
  priority = {prio3},
  ranking  = {rank1},
  type     = {Journal Article},
}

﻿
@Article{BinKamilin2021_MultiTaskLearningBasedTaskSchedulingSwitcherforaResourceConstrainedIoTSystem,
  author     = {Bin Kamilin, Mohd Hafizuddin and Bin Ahmadon, Mohd Anuaruddin and Yamaguchi, Shingo},
  journal    = {Information},
  title      = {Multi-Task Learning-Based Task Scheduling Switcher for a Resource-Constrained IoT System},
  year       = {2021},
  code       = {https://github.com/hafiz-kamilin/research_taskSchedulingSwitcher},
  number     = {4},
  pages      = {150},
  volume     = {12},
  abstract   = {In this journal, we proposed a novel method of using multi-task learning to switch the scheduling algorithm. With multi-task learning to change the scheduling algorithm inside the scheduling framework, the scheduling framework can create a scheduler with the best task execution optimization under the computation deadline. With the changing number of tasks, the number of types of resources taken, and computation deadline, it is hard for a single scheduling algorithm to achieve the best scheduler optimization while avoiding the worst-case time complexity in a resource-constrained Internet of Things (IoT) system due to the trade-off in computation time and optimization in each scheduling algorithm. Furthermore, different hardware specifications affect the scheduler computation time differently, making it hard to rely on Big-O complexity as a reference. With multi-task learning to profile the scheduling algorithm behavior on the hardware used to compute the scheduler, we can identify the best scheduling algorithm. Our benchmark result shows that it can achieve an average of 93.68% of accuracy in meeting the computation deadline, along with 23.41% of average optimization. Based on the results, our method can improve the scheduling of the resource-constrained IoT system.},
  doi        = {10.3390/info12040150},
  file       = {:Bin Kamilin-2021-Multi-Task Learning-Based Tas.pdf:PDF},
  groups     = {Multi-Task Learning, Scheduling, Internet of Things},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {read},
  type       = {Journal Article},
}

@Article{Ruder2017_AnOverviewofMultiTaskLearninginDeepNeuralNetworks,
  author     = {Ruder, Sebastian},
  journal    = {arXiv preprint arXiv:1706.05098},
  title      = {An overview of multi-task learning in deep neural networks},
  year       = {2017},
  abstract   = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
  eprint     = {1706.05098},
  file       = {:Ruder-2017-An overview of multi-task learning.pdf:PDF},
  groups     = {Multi-Task Learning, Classification},
  priority   = {prio2},
  ranking    = {rank4},
  readstatus = {read},
  type       = {Journal Article},
}

@Article{Li2022_CompetitionLevelCodeGenerationwithAlphaCode,
  author   = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julia and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agusti and Hubert, Thomas and Choy, Peter and d'Autume, Cyprien de Masson and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Robson, Esme Sutherland and Kohli, Pushmeet and Freitas, Nando de and Kavukcuoglu, Koray and Vinyals, Oriol},
  journal  = {DeepMind 2022},
  title    = {Competition-Level Code Generation with AlphaCode},
  year     = {2022},
  code     = {https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode},
  abstract = {Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.},
  file     = {:Yujia Li-2022-Competition-Level Code Generatio.pdf:PDF},
  groups   = {Natural Language Processing, Translation},
  priority = {prio1},
  ranking  = {rank4},
  type     = {Journal Article},
  url      = {https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode},
}

﻿
@Article{Biessmann2019_DataWigMissingValueImputationforTables,
  author   = {Biessmann, Felix and Rukat, Tammo and Schmidt, Philipp and Naidu, Prathik and Schelter, Sebastian and Taptunov, Andrey and Lange, Dustin and Salinas, David},
  journal  = {J. Mach. Learn. Res.},
  title    = {DataWig: Missing Value Imputation for Tables},
  year     = {2019},
  code     = {https://github.com/awslabs/datawig},
  number   = {175},
  pages    = {1-6},
  volume   = {20},
  abstract = {With the growing importance of machine learning (ML) algorithms for practical applications, reducing data quality problems in ML pipelines has become a major focus of research. In many cases missing values can break data pipelines which makes completeness one of the most impactful data quality challenges. Current missing value imputation methods are focusing on numerical or categorical data and can be difficult to scale to datasets with millions of rows. We release DataWig, a robust and scalable approach for missing value imputation that can be applied to tables with heterogeneous data types, including unstructured text. DataWig combines deep learning feature extractors with automatic hyperparameter tuning. This enables users without a machine learning background, such as data engineers, to impute missing values with minimal effort in tables with more heterogeneous data types than supported in existing libraries, while requiring less glue code for feature engineering and offering more flexible modelling options. We demonstrate that DataWig compares favourably to existing imputation packages. Source code, documentation, and unit tests for this package are available at: https://github.com/awslabs/datawig},
  file     = {:Biessmann-2019-DataWig_ Missing Value Imputati.pdf:PDF},
  groups   = {Missing Data},
  priority = {prio2},
  ranking  = {rank3},
  type     = {Journal Article},
}

﻿
@Article{Whittington2021_RelatingTransformerstoModelsandNeuralRepresentationsoftheHippocampalFormation,
  author   = {Whittington, James CR and Warren, Joseph and Behrens, Timothy EJ},
  journal  = {arXiv preprint arXiv:2112.04035},
  title    = {Relating transformers to models and neural representations of the hippocampal formation},
  year     = {2021},
  abstract = {Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain. One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind. In this work, we show that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place and grid cells. Furthermore, we show that this result is no surprise since it is closely related to current hippocampal models from neuroscience. We additionally show the transformer version offers dramatic performance gains over the neuroscience version. This work continues to bind computations of artificial and brain networks, offers a novel understanding of the hippocampal-cortical interaction, and suggests how wider cortical areas may perform complex tasks beyond current neuroscience models such as language comprehension.},
  eprint   = {2112.04035},
  file     = {:Whittington-2021-Relating transformers to mode.pdf:PDF},
  groups   = {Miscellaneous},
  priority = {prio3},
  ranking  = {rank1},
  type     = {Journal Article},
}

﻿
@InProceedings{Wang2021_RealEsrganTrainingRealWorldBlindSuperResolutionwithPureSyntheticData,
  author    = {Wang, Xintao and Xie, Liangbin and Dong, Chao and Shan, Ying},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  title     = {Real-esrgan: Training real-world blind super-resolution with pure synthetic data},
  year      = {2021},
  code      = {https://github.com/xinntao/Real-ESRGAN},
  pages     = {1905-1914},
  abstract  = {Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly.},
  doi       = {10.1109/iccvw54120.2021.00217},
  file      = {:Wang-2021-Real-esrgan_ Training real-world bli.pdf:PDF},
  groups    = {Generative adversarial networks, Upscaler, Upscaling Image/Video},
  priority  = {prio2},
  ranking   = {rank3},
  type      = {Conference Proceedings},
}

﻿
@Article{Anderson2022_FindingtheOptimalHumanStrategyforWordleUsingMaximumCorrectLetterProbabilitiesandReinforcementLearning,
  author   = {Anderson, Benton J and Meyer, Jesse G},
  journal  = {arXiv preprint arXiv:2202.00557},
  title    = {Finding the optimal human strategy for Wordle using maximum correct letter probabilities and reinforcement learning},
  year     = {2022},
  abstract = {Wordle is an online word puzzle game that gained viral popularity in January 2022. The goal is to guess a hidden five letter word. After each guess, the player gains information about whether the letters they guessed are present in the word, and whether they are in the correct position. Numerous blogs have suggested guessing strategies and starting word lists that improve the chance of winning. Optimized algorithms can win 100% of games within five of the six allowed trials. However, it is infeasible for human players to use these algorithms due to an inability to perfectly recall all known 5-letter words and perform complex calculations that optimize information gain. Here, we present two different methods for choosing starting words along with a framework for discovering the optimal human strategy based on reinforcement learning. Human Wordle players can use the rules we discover to optimize their chance of winning.},
  eprint   = {2202.00557},
  file     = {:Anderson-2022-Finding the optimal human strate.pdf:PDF},
  groups   = {Reinforcement Learning, Prediction},
  priority = {prio3},
  ranking  = {rank2},
  type     = {Journal Article},
}

﻿
@Article{Pan2022_MachineLearningBasedWhiteHatWormLauncherinBotnetDefenseSystem,
  author   = {Pan, Xiangnan and Yamaguchi, Shingo and Kageyama, Taku and Kamilin, Mohd Hafizuddin Bin},
  journal  = {International Journal of Software Science and Computational Intelligence},
  title    = {Machine-Learning-Based White-Hat Worm Launcher in Botnet Defense System},
  year     = {2022},
  issn     = {1942-9045
1942-9037},
  number   = {1},
  pages    = {1-14},
  volume   = {14},
  abstract = {This article proposes a white-hat worm launcher based on machine learning (ML) adaptable to large-scale IoT network for Botnet Defense System (BDS). BDS is a cyber-security system that uses white-hat worms to exterminate malicious botnets. White-hat worms defend an IoT system against malicious bots, the BDS decides the number of white-hat worms, but there is no discussion on the white-hat worms' deployment in IoT network. Therefore, the authors propose a machine-learning-based launcher to launch the white-hat worms effectively along with a divide and conquer algorithm to deploy the launcher to large-scale IoT networks. Then the authors modeled BDS and the launcher with agent-oriented Petri net and confirmed the effect through the simulation of the PN2 model. The result showed that the proposed launcher can reduce the number of infected devices by about 30-40%.},
  doi      = {10.4018/ijssci.291713},
  file     = {:Pan-2022-Machine-Learning-Based White-Hat Worm.pdf:PDF},
  groups   = {Multi-Task Learning, Internet Security},
  priority = {prio1},
  ranking  = {rank5},
  type     = {Journal Article},
  url      = {https://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/IJSSCI.291713},
}

@Article{LeCun2022_APathTowardsAutonomousMachineIntelligenceVersion0.9.220220627,
  author   = {LeCun, Yann},
  title    = {A Path Towards Autonomous Machine Intelligence Version 0.9. 2, 2022-06-27},
  year     = {2022},
  file     = {:LeCun-2022-a_path_towards_autonomous_mach.pdf:PDF},
  groups   = {Artificial General Intelligence},
  priority = {prio1},
  ranking  = {rank4},
  url      = {https://openreview.net/forum?id=BZ5a1r-kVsf},
}

﻿
@Sourcecode{Peng2019_Anime4KaHighQualityRealTimeUpscalerforAnimeVideo,
  author         = {Peng, B.},
  title          = {Anime4K: A High-Quality Real Time Upscaler for Anime Video},
  year           = {2019},
  code           = {https://github.com/bloc97/Anime4K},
  lastupdatedate = {26 November 2021},
  abstract       = {Anime4K is a set of open-source, high-quality real-time anime upscaling/denoising algorithms that can be implemented in any programming language.

The simplicity and speed of Anime4K allows the user to watch upscaled anime in real time, as we believe in preserving original content and promoting freedom of choice for all anime fans. Re-encoding anime into 4K should be avoided as it is non-reversible, potentially damages original content by introducing artifacts, takes up to O(n^2) more disk space and more importantly, does so without any meaningful decrease in entropy (lost information is lost).},
  file           = {:Peng-2019-Anime4K_ A High-Quality Real Time Up.pdf:PDF},
  groups         = {Upscaler, Upscaling Image/Video},
  month          = {11/26/2021},
  priority       = {prio3},
  publisher      = {Github},
  ranking        = {rank2},
  readstatus     = {read},
  type           = {Sourcecode},
  url            = {https://github.com/bloc97/Anime4K},
}

﻿
@InProceedings{Dong_AcceleratingtheSuperResolutionConvolutionalNeuralNetwork,
  author    = {Dong, Chao and Loy, Chen Change and Tang, Xiaoou},
  booktitle = {European conference on computer vision},
  title     = {Accelerating the super-resolution convolutional neural network},
  year      = {2016},
  code      = {https://github.com/yjn870/FSRCNN-pytorch, https://github.com/Saafke/FSRCNN_Tensorflow},
  pages     = {391-407},
  publisher = {Springer},
  abstract  = {As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt smaller filter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with even superior restoration quality. Further, we present the parameter settings that can achieve real-time performance on a generic CPU while still maintaining good performance. A corresponding transfer strategy is also proposed for fast training and testing across different upscaling factors.},
  doi       = {10.1007/978-3-319-46475-6_25},
  file      = {:Dong-2016-Accelerating the super-resolution co.pdf:PDF},
  groups    = {Upscaler, Upscaling Image/Video},
  priority  = {prio2},
  ranking   = {rank3},
  type      = {Conference Proceedings},
}

﻿
@Article{Pan2021_IncoherentReconstructionFreeObjectRecognitionwithMaskBasedLenslessOpticsandtheTransformer,
  author   = {Pan, Xiuxi and Chen, Xiao and Nakamura, Tomoya and Yamaguchi, Masahiro},
  journal  = {Optics Express},
  title    = {Incoherent reconstruction-free object recognition with mask-based lensless optics and the Transformer},
  year     = {2021},
  number   = {23},
  pages    = {37962-37978},
  volume   = {29},
  abstract = {A mask-based lensless camera adopts a thin mask to optically encode the scene and records the encoded pattern on an image sensor. The lensless camera can be thinner, lighter and cheaper than the lensed camera. But additional computation is required to reconstruct an image from the encoded pattern. Considering that the significant application of the lensless camera could be inference, we propose to perform object recognition directly on the encoded pattern. Avoiding image reconstruction not only saves computational resources but also averts errors and artifacts in reconstruction. We theoretically analyze multiplexing property in mask-based lensless optics which maps local information in the scene to overlapping global information in the encoded pattern. To better extract global features, we propose a simplified Transformer-based architecture. This is the first time to study Transformer-based architecture for encoded pattern recognition in mask-based lensless optics. In the optical experiment, the proposed system achieves 91.47&#x0025; accuracy on the Fashion MNIST and 96.64&#x0025; ROC AUC on the cats-vs-dogs dataset. The feasibility of physical object recognition is also evaluated.},
  doi      = {10.1364/OE.443181},
  file     = {:Pan-2022-Image reconstruction with transformer.pdf:PDF},
  groups   = {Convolutional Neural Network, Reconstruction},
  keywords = {Feature extraction Image reconstruction Image sensors Optical sensing Pattern recognition Stochastic gradient descent},
  priority = {prio2},
  ranking  = {rank3},
  type     = {Journal Article},
  url      = {http://opg.optica.org/oe/abstract.cfm?URI=oe-29-23-37962},
}

@Article{Pan2022_ImageReconstructionwithTransformerforMaskBasedLenslessImaging,
  author   = {Pan, Xiuxi and Chen, Xiao and Takeyama, Saori and Yamaguchi, Masahiro},
  journal  = {Optics Letters},
  title    = {Image reconstruction with transformer for mask-based lensless imaging},
  year     = {2022},
  number   = {7},
  pages    = {1843-1846},
  volume   = {47},
  abstract = {A mask-based lensless camera optically encodes the scene with a thin mask and reconstructs the image afterward. The improvement of image reconstruction is one of the most important subjects in lensless imaging. Conventional model-based reconstruction approaches, which leverage knowledge of the physical system, are susceptible to imperfect system modeling. Reconstruction with a pure data-driven deep neural network (DNN) avoids this limitation, thereby having potential to provide a better reconstruction quality. However, existing pure DNN reconstruction approaches for lensless imaging do not provide a better result than model-based approaches. We reveal that the multiplexing property in lensless optics makes global features essential in understanding the optically encoded pattern. Additionally, all existing DNN reconstruction approaches apply fully convolutional networks (FCNs) which are not efficient in global feature reasoning. With this analysis, for the first time to the best of our knowledge, a fully connected neural network with a transformer for image reconstruction is proposed. The proposed architecture is better in global feature reasoning, and hence enhances the reconstruction. The superiority of the proposed architecture is verified by comparing with the model-based and FCN-based approaches in an optical experiment.},
  doi      = {10.1364/OL.455378},
  file     = {:Pan-2021-Incoherent reconstruction-free object.pdf:PDF},
  groups   = {Convolutional Neural Network, Reconstruction},
  keywords = {Fresnel zones Image quality Image reconstruction Image sensors Neural networks Reconstruction algorithms},
  priority = {prio2},
  ranking  = {rank3},
  type     = {Journal Article},
  url      = {http://opg.optica.org/ol/abstract.cfm?URI=ol-47-7-1843},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Machine Learning\;2\;1\;0x8a8a8aff\;ROBOT\;Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks. \;;
2 StaticGroup:Model\;2\;1\;0x8a8a8aff\;TOY_BRICK\;A machine learning model is a file that has been trained to recognize certain types of patterns.\;;
3 StaticGroup:Autoencoder\;0\;1\;0x8a8a8aff\;\;An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning).\;;
3 StaticGroup:Gated Recurrent Unit\;0\;1\;0x8a8a8aff\;\;Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks.\;;
3 StaticGroup:Graph Neural Network\;0\;1\;0x8a8a8aff\;\;A Graph neural network (GNN) is a class of artificial neural networks for processing data that can be represented as graphs.\;;
3 StaticGroup:Natural Language Processing\;0\;1\;0x8a8a8aff\;\;Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\;;
3 StaticGroup:Multi-Task Learning\;0\;1\;0x8a8a8aff\;\;Multi-task learning is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks.\;;
3 StaticGroup:Generative adversarial networks\;0\;1\;0x8a8a8aff\;\;A generative adversarial network is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in June 2014. Two neural networks contest with each other in a game. Given a tr